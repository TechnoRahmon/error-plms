{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch Setup\n",
    "\n",
    "For this project, we will be building an open-domain question answering system. There are three major components to such a system:\n",
    "\n",
    "- Database\n",
    "\n",
    "- Retriever\n",
    "\n",
    "- Reader\n",
    "\n",
    "In this notebook we will setup the first part, the database - where we will be using Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 30.7/232.6 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 112.6/232.6 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 194.6/232.6 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 232.6/232.6 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf2\n",
      "Successfully installed pypdf2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import pdf file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_reader = PdfReader('./data/Error-Analysis-PLMs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the chunks text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by replacing unwanted text with desired replacements.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input text to be preprocessed.\n",
    "    - replacements (dict): A dictionary where keys are the unwanted text to be replaced,\n",
    "                           and values are the replacements for each key.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    # Define replacements (e.g., replace '\\xa0' with a regular space)\n",
    "    replacements = {'\\xa0': ' ', '\\n':' '}\n",
    "\n",
    "    for old_text, new_text in replacements.items():\n",
    "        text = text.replace(old_text, new_text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the pdf text for all pages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "pages_text = [preprocess_text(page.extract_text()) for page in pdf_reader.pages]\n",
    "\n",
    "#pdf_chunks = list(chain.from_iterable(pages_text))[2:]\n",
    "len(pages_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vol.:(0123456789)Human-Centric Intelligent Systems  https://doi.org/10.1007/s44230-024-00061-7 RESEARCH ARTICLE Error Analysis of Pretrained Language Models (PLMs)  in English‑to‑Arabic Machine Translation Hend Al‑Khalifa1,3  · Khaloud Al‑Khalefah2 · Hesham Haroon3 Received: 3 October 2023 / Accepted: 4 January 2024  © The Author(s) 2024 Abstract Advances in neural machine translation utilizing pretrained language models (PLMs) have shown promise in improving the  translation quality between diverse languages. However, translation from English to languages with complex morphology,  such as Arabic, remains challenging. This study investigated the prevailing error patterns of state-of-the-art PLMs when  translating from English to Arabic across different text domains. Through empirical analysis using automatic metrics (chrF,  BERTScore, COMET) and manual evaluation with the Multidimensional Quality Metrics (MQM) framework, we compared  Google Translate and five PLMs (Helsinki, Marefa, Facebook, GPT-3.5-turbo, and GPT-4). Key findings provide valuable  insights into current PLM limitations in handling aspects of Arabic grammar and vocabulary while also informing future  improvements for advancing English–Arabic machine translation capabilities and accessibility. Keywords Machine translation · Pretrained large language models · Translation studies · GPT · Arabic language 1 Introduction The digital era has brought about a significant shift in com- munication and interaction across linguistic and cultural  boundaries. With the rise of global interconnectedness, the  ability to translate between languages, particularly those  using non-Latin scripts, such as Arabic, has become increas - ingly important. However, many writing systems beyond  the Latin alphabet face barriers to digital accessibility and  participation [1 ]. Artificial Intelligence (AI), particularly in  the field of Neural Machine Translation (NMT), has shown  promise in bridging these linguistic divides [2, 3]. How - ever, translating between languages with profound structural  and script differences, such as English and Arabic, presents  unique challenges [4 , 5].Pretrained Language Models (PLMs) are at the forefront  of Natural Language Processing (NLP) research and have  significantly enhanced machine translation capabilities [6 ].  Despite their advancements, these models still struggle to  achieve high levels of accuracy and fluency in English-to- Arabic translation [7 ]. This limitation hampers effective  communication and cooperation across English and Arabic- speaking cultures, which is crucial in areas such as trade,  diplomacy, and knowledge exchange. This study aims to  investigate the error patterns in state-of-the-art PLMs when  translating from English to Arabic. Our objectives will be: • To evaluate these models in a parallel corpus of English– Arabic sentences. • To identify common error patterns and explore their pos- sible causes. • To provide insights into the limitations of current PLMs. This study targets academics, NLP practitioners, and poli- cymakers interested in leveraging AI for language transla- tion. Our findings provide valuable insights into the current  limitations of PLMs in terms of English-to-Arabic transla- tion quality. The analysis intends to offer guidance for future  research efforts focused on advancing machine translation  capabilities for the English–Arabic language pairs. Addition - ally, by elucidating the existing challenges in cross-lingual  * Hend Al-Khalifa   hendk@ksu.edu.sa  Khaloud Al-Khalefah   kholodalkhalifah@gmail.com 1 Information Technology Department, King Saud University,  Riyadh, Saudi Arabia 2 College of Language and Translation, Al-Imam Muhammad  Ibn Saud Islamic University, Riyadh, Saudi Arabia 3 iWAN Research Group, King Saud University, Riyadh,  Saudi Arabia'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_name': 'docker-cluster',\n",
       " 'status': 'green',\n",
       " 'timed_out': False,\n",
       " 'number_of_nodes': 1,\n",
       " 'number_of_data_nodes': 1,\n",
       " 'active_primary_shards': 3,\n",
       " 'active_shards': 3,\n",
       " 'relocating_shards': 0,\n",
       " 'initializing_shards': 0,\n",
       " 'unassigned_shards': 0,\n",
       " 'delayed_unassigned_shards': 0,\n",
       " 'number_of_pending_tasks': 0,\n",
       " 'number_of_in_flight_fetch': 0,\n",
       " 'task_max_waiting_in_queue_millis': 0,\n",
       " 'active_shards_percent_as_number': 100.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('http://localhost:9200/_cluster/health').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green open .geoip_databases vK8uAcb1QJmzR1OndcuG8A 1 0 41 0 38.4mb 38.4mb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(requests.get('http://localhost:9200/_cat/indices').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delete error_plms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdul\\NLP-course\\.venv\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Create a connection\n",
    "es = Elasticsearch([{'host':'localhost', 'port':9200}])\n",
    "\n",
    "# Delete the index\n",
    "es.indices.delete(index='error_plms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize a new index error_plms which we will use to store our Error PLMs dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "doc_store = ElasticsearchDocumentStore(\n",
    "    host='localhost',\n",
    "    username='', password='',\n",
    "    index='error_plms'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green  open .geoip_databases vK8uAcb1QJmzR1OndcuG8A 1 0 41 0 38.4mb 38.4mb\n",
      "yellow open label            zi9jIquWRaCd7ANkED4aLw 1 1  0 0   227b   227b\n",
      "yellow open error_plms       u53WxAAFTDq-Oe3-zZW9cQ 1 1  0 0   227b   227b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(requests.get('http://localhost:9200/_cat/indices').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to format our data into a list of dictionaries before passing it along to Elasticsearch. We will create the format:\n",
    "\n",
    "```\n",
    "    {\n",
    "        'content': '<paragraph>',\n",
    "        'meta': {\n",
    "            'source': 'meditations'\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = [\n",
    "    {\n",
    "        'content': paragraph,\n",
    "        'meta': {\n",
    "            'source': 'Human-Centric'\n",
    "        }\n",
    "    } for paragraph in pages_text\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Vol.:(0123456789)Human-Centric Intelligent Systems  https://doi.org/10.1007/s44230-024-00061-7 RESEARCH ARTICLE Error Analysis of Pretrained Language Models (PLMs)  in English‑to‑Arabic Machine Translation Hend Al‑Khalifa1,3  · Khaloud Al‑Khalefah2 · Hesham Haroon3 Received: 3 October 2023 / Accepted: 4 January 2024  © The Author(s) 2024 Abstract Advances in neural machine translation utilizing pretrained language models (PLMs) have shown promise in improving the  translation quality between diverse languages. However, translation from English to languages with complex morphology,  such as Arabic, remains challenging. This study investigated the prevailing error patterns of state-of-the-art PLMs when  translating from English to Arabic across different text domains. Through empirical analysis using automatic metrics (chrF,  BERTScore, COMET) and manual evaluation with the Multidimensional Quality Metrics (MQM) framework, we compared  Google Translate and five PLMs (Helsinki, Marefa, Facebook, GPT-3.5-turbo, and GPT-4). Key findings provide valuable  insights into current PLM limitations in handling aspects of Arabic grammar and vocabulary while also informing future  improvements for advancing English–Arabic machine translation capabilities and accessibility. Keywords Machine translation · Pretrained large language models · Translation studies · GPT · Arabic language 1 Introduction The digital era has brought about a significant shift in com- munication and interaction across linguistic and cultural  boundaries. With the rise of global interconnectedness, the  ability to translate between languages, particularly those  using non-Latin scripts, such as Arabic, has become increas - ingly important. However, many writing systems beyond  the Latin alphabet face barriers to digital accessibility and  participation [1 ]. Artificial Intelligence (AI), particularly in  the field of Neural Machine Translation (NMT), has shown  promise in bridging these linguistic divides [2, 3]. How - ever, translating between languages with profound structural  and script differences, such as English and Arabic, presents  unique challenges [4 , 5].Pretrained Language Models (PLMs) are at the forefront  of Natural Language Processing (NLP) research and have  significantly enhanced machine translation capabilities [6 ].  Despite their advancements, these models still struggle to  achieve high levels of accuracy and fluency in English-to- Arabic translation [7 ]. This limitation hampers effective  communication and cooperation across English and Arabic- speaking cultures, which is crucial in areas such as trade,  diplomacy, and knowledge exchange. This study aims to  investigate the error patterns in state-of-the-art PLMs when  translating from English to Arabic. Our objectives will be: • To evaluate these models in a parallel corpus of English– Arabic sentences. • To identify common error patterns and explore their pos- sible causes. • To provide insights into the limitations of current PLMs. This study targets academics, NLP practitioners, and poli- cymakers interested in leveraging AI for language transla- tion. Our findings provide valuable insights into the current  limitations of PLMs in terms of English-to-Arabic transla- tion quality. The analysis intends to offer guidance for future  research efforts focused on advancing machine translation  capabilities for the English–Arabic language pairs. Addition - ally, by elucidating the existing challenges in cross-lingual  * Hend Al-Khalifa   hendk@ksu.edu.sa  Khaloud Al-Khalefah   kholodalkhalifah@gmail.com 1 Information Technology Department, King Saud University,  Riyadh, Saudi Arabia 2 College of Language and Translation, Al-Imam Muhammad  Ibn Saud Islamic University, Riyadh, Saudi Arabia 3 iWAN Research Group, King Saud University, Riyadh,  Saudi Arabia',\n",
       "  'meta': {'source': 'Human-Centric'}},\n",
       " {'content': ' Human-Centric Intelligent Systems AI systems, we hope to highlight the broader significance of  progress in this space to achieve equitable global participa- tion in the exchange of ideas. The rest of the paper is organized as follows: Sect.  2 pro- vides background and related work, discussing Pretrained  Language Models, English-to-Arabic machine transla- tion, and error analysis. Section  3 details the methodology,  including the dataset, PLM selection and training, and the  evaluation metrics used. Section  4 presents the results and  analysis, comprising a model performance comparison, error  classification, and patterns. Section 5 concludes the paper by  summarizing the key findings and implications for improv - ing English-to-Arabic translation. Finally, Sect. 6 offers  recommendations and future work, highlighting potential  avenues for enhancing the PLM performance in this lan- guage pair. 2  Background and Related Work Machine translation, pioneered in the 1950s, initially saw  success with the Georgetown-IBM experiment, utilizing  statistical algorithms for Russian-to-English translations.  Over time, this field has diversified into rule-based systems,  statistical approaches, neural networks, and Large Language  Models, each with unique methods for learning and translat - ing languages. These systems, especially neural networks  and LLMs, continuously improve their translation accuracy  by training on extensive datasets and grasping the underlying  knowledge in the text [8 ]. In this section, we briefly discuss three topics related  to our research: English-to-Arabic machine translation  research, Pretrained Language Models (PLMs) in machine  translation, and Error Analysis in machine translation. 2.1  English‑to‑Arabic Machine Translation Machine translation from English to Arabic has been an  active area of research for several years. Researchers have  explored various approaches to tackling the challenges  inherent in this task, including differences in grammar,  word order, and vocabulary between the two languages.  In addition, many survey papers have been published  on Arabic linguistic characteristics and translation chal- lenges. For instance, Ameur et al. [9 ] summarized critical  research on Arabic MT and the available tools/resources  for building Arabic MT systems. The survey discussed the  state of the field and provided insights into future Arabic  MT research directions. In addition, Zakraoui et al. [10]  provided a comprehensive review comparing different  NMT approaches for Arabic-English translations. They  discussed approaches addressing linguistic and techni- cal challenges, and demonstrated success over traditional methods. Their results will serve to update researchers on  resources for improving Arabic MT, including corpora,  toolkits, techniques, and models. Rule-based methods, which rely on handcrafted rules  to analyze the source language and generate the target  language, have shown some success. Farhat and Al-Taani  [11] developed a rule-based system that could translate  simple English sentences into Arabic with an accuracy of  85.71%. Similarly, Alawneh et al. [12] combined rule-based  and example-based English-to-Arabic machine translation  using parsing and a hybrid methodology to handle order - ing and agreement. They evaluated their approach on 250  test samples, and the results achieved 97.2% precision on  average. Also, Al-Rukban and Saudagar [13] evaluated three  commercial English-to-Arabic systems, Google Translate,  Bing Translator, and Golden Alwafi, and found that Golden  Alwafi achieved the highest BLEU score, indicating the most  human-like translations. Although rule-based methods are  straightforward, they require extensive time to develop and  maintain. Neural-driven methods, including those based on neu- ral networks and PLMs, have become increasingly popu- lar. Akeel and Mishra [14] developed an English-to-Arabic  translator using both rule-based and neural network meth- ods, achieving scores of 0.6029 on the n-gram BLUE score  and 0.8221 on the METEOR metric. Aljohany et al. [ 15]  proposed a bidirectional model for the translation between  Arabic and English. This model employs a Long Short- Term Memory (LSTM) encoder-decoder with an attention  mechanism to address the performance degradation linked  to increased input sentence length. The integration of LSTM  and attention mechanisms improves the translation accuracy,  as substantiated by the experimental results, which dem - onstrate improved translation precision and reduced loss.  Some researchers have focused on the challenges of Eng- lish-to-Arabic translation and have suggested directions for  future work. Aref et al. [16] outlined a multi-level approach  to machine translation and reviewed the state of English- to-Arabic translation, suggesting the use of AI techniques  like knowledge representation to build a prototype system.  In contrast, Nagoudi et al. [17] developed TURJUMAN, a  toolkit leveraging the Transformer AraT5 model, and trans- lated 20 languages into Modern Standard Arabic (MSA). Table  1 summarizes the main rule-based and neural  approaches explored for English-to-Arabic machine trans- lation. For each approach, key published works are high- lighted, along with their main findings, limitations, and  open gaps in the research. This provides a concise overview  of the current state of English-to-Arabic MT literature to  identify promising future research directions. We can see  that the key gaps in English-to-Arabic translation include  insufficient parallel data, challenges with Arabic morphol- ogy, and linguistic divergence. Our research utilizes a new ',\n",
       "  'meta': {'source': 'Human-Centric'}},\n",
       " {'content': 'Human-Centric Intelligent Systems  English–Arabic parallel corpus and several PLMs models on  this data to adapt it and assess the translation performance. 2.2  Pretrained Language Models (PLMs) in Machine  Translation Pretrained language models are neural network models  trained on large amounts of text data in an unsupervised  manner. This pre-training process allows the models to  learn general linguistic knowledge from the data, including  semantics, syntax, and relationships between words. PLMs  can then be fine-tuned on downstream supervised tasks, such  as text classification, question answering, text generation  and machine translation. Well-known PLMs include BERT,  GPT-2, and RoBERTa. PLMs have shown promising results in Machine Trans- lation. For instance, BART (Bidirectional and Auto- Regressive Transformer) is a PLM that have been used for  MT and shown to improve the performance of MT systems [18]. Chronopoulou et al. [19] used a language model pre- trained on two languages with large monolingual data to  initialize an unsupervised neural machine translation sys- tem, which yielded state-of-the-art results. Edunov et al.  [20] have examined different strategies to integrate pre- trained representations into sequence-to-sequence models  and applied it to neural machine translation. PLMs have  also been used for low-resource machine translation [21],  sign language translation [22], and code-mixed Hinglish- to-English machine translation [23]. However, the suc- cessful construction of such models often requires large  amounts of data and computational resources [24]. Table  2 summarizes the key gaps in using PLMs for  machine translation, including the lack of models tailored  for particular language pairs, such as English–Arabic,  insufficient data and computing access for low-resource  settings, and challenges in scaling cross-lingual transfer  to many languages. Our research seeks to assess the value  of PLMs, even with limited resources, and evaluate their  quality.Table 1  Summary of key approaches, findings, limitations, and open gaps in English-to-Arabic machine translation literature Approach Key works Findings Limitations Rule- based Farhat and Al-Taani [11]: Rule-based  system for simple English-to-Arabic  translation, 85.71% accuracy Alawneh et al. [12]: Combined rule-based  and example-based approach, 97.2%  precision Al-Rukban and Saudagar [13]: Evaluated  commercial systems, Golden Alwafi had  highest BLEUCan achieve good accuracy and precision for simple  sentences Requires extensive manual effort for rulesDo not scale well  to complex  sentences Hard to maintain  rules over time Neural methods Akeel and Mishra [14]: Combined rule- based and neural, BLEU of 0.6029 Aljohany et al. [15]: LSTM encoder- decoder with attention for better long  sentence translation Nagoudi et al. [17]: TURJUMAN toolkit  with Transformer modelNeural models outperform rule-based Attention mechanisms help with long sentencesLimited focus on  English–Arabic  specifically More complex  linguistic chal- lenges not fully  solved Table 2  Summary of key studies utilizing PLMs for machine translation Approach Key works Findings Limitations PLM for MT BART model [18] improves MT perfor - mance  Chronopoulou et al. [19]: pretrained LM  to initialize unsupervised NMT, SOTA  resultsPLMs capture linguistic knowledge use- ful for MT Can improve supervised and unsuper - vised MTRequire large data and compute  resources [24] Low-resource MT PLMs used successfully for low-resource  MT [21]Help mitigate data scarcity challenges Still limited by small data size Multilingual MT Edunov et al. [20]: integrate multilingual  pretrained representations into MT  modelsLeverage cross-lingual transfer learning Difficult to scale to many languages',\n",
       "  'meta': {'source': 'Human-Centric'}}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we simply write our data to Elasticsearch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_store.write_documents(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 14,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('http://localhost:9200/error_plms/_count').json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
