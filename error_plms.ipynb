{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haystack pipeline\n",
    "\n",
    "connect to the elasticsearch document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "doc_store = ElasticsearchDocumentStore(\n",
    "    host='localhost',\n",
    "    username='', password='',\n",
    "    index='error_plms'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize our retriever and reader models. We will be using `deepset/bert-base-cased-squad2` as our reader model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import FARMReader, BM25Retriever\n",
    "retriever = BM25Retriever(doc_store)\n",
    "reader = FARMReader(model_name_or_path='deepset/bert-base-cased-squad2',\n",
    "                    context_window_size=1500,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**initialize our ODQA pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "qa = ExtractiveQAPipeline(reader=reader, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response_obj):\n",
    "    query = response_obj['query']\n",
    "    answers = response_obj['answers']\n",
    "    if  len(answers) > 0:\n",
    "        print(f'Q: {query}')\n",
    "        _ = [print(f'A{index+1}: {answer.answer}') for index, answer in enumerate(answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ask(query=\"\"):\n",
    "    if len(query) > 0:\n",
    "        response = qa.run(query=query, params={\"Reader\": {\"top_k\": 3}})\n",
    "        extract_answer(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdul\\NLP-course\\.venv\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:27<00:00, 13.83s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: what is PLMs ?\n",
      "A1: pretrained language models\n",
      "A2: Pretrained Language Models\n",
      "A3: Pretrained Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Ask(\"what is PLMs ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 2/2 [00:32<00:00, 16.31s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: what are Pretrained  language  models ?\n",
      "A1: languages with limited corpora for unsuper - vised NMT\n",
      "A2: neural network models  trained on large amounts of text data in an unsupervised  manner\n",
      "A3: a state-of-the-art walkthrough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Ask(\"what are Pretrained  language  models ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 2/2 [00:28<00:00, 14.45s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: what is the objectives when translating from English to Arabic ?\n",
      "A1: assess the translation performance\n",
      "A2: improves the translation accuracy\n",
      "A3: better long  sentence translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Ask(\"what is the objectives when translating from English to Arabic ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 2/2 [00:30<00:00, 15.22s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: what is Conclusion and Future Work ?\n",
      "A1: advancing machine translation  capabilities for the English–Arabic language pairs\n",
      "A2: state-of-the-art pretrained language models\n",
      "A3: Sect. 6 offers  recommendations and future work, highlighting potential  avenues for enhancing the PLM performance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Ask(\"what is Conclusion and Future Work ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
